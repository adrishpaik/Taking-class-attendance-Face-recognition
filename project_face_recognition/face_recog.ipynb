{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73bb7417-02c0-4c51-ace7-ab2036f9f50f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name please: Adrish\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Capturing video from the internal webcam\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "# Loading the pre-trained face detection model (Haar Cascade)\n",
    "facedetect = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "faces_data = []\n",
    "name = input(\"Enter your name please:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c84da-ac9a-47e6-b779-905daad2fba7",
   "metadata": {},
   "source": [
    "gray: This is the input image in grayscale. The face detection algorithm typically works on grayscale images because color information is not necessary for detecting face structures.\n",
    "\n",
    "1.3: This is the scale factor. It specifies how much the image size is reduced at each image scale. A value of 1.3 means the image is reduced by 30% for each pyramid layer. This helps the algorithm detect faces at different scales (sizes) within the image. Smaller values (close to 1) mean finer detection but at the cost of speed.\n",
    "\n",
    "5: This is the minNeighbors parameter. It specifies how many neighbors each candidate rectangle (a detected face) should have to retain it. A higher value (like 5) means that the algorithm is more selective, reducing false positives by requiring that a face must be surrounded by a certain number of other detected objects (potential faces) to be considered valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "236b43e8-1f65-46cd-be32-65f33605a8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while True:\n",
    "    #ret denotes camera is ok or not \n",
    "    ret,frame = video.read() \n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) #converting\n",
    "    \n",
    "    # Detecting faces in the grayscale image\n",
    "    faces = facedetect.detectMultiScale(gray,1.3,5)\n",
    "    \n",
    "    for (x,y,w,h) in faces:#it encoded rectangle\n",
    "        crop_image = frame[y:y+h,x:x+w,:] #(row,colmn,channel)\n",
    "        resized_image = cv2.resize(crop_image,(50,50))\n",
    "        if len(faces_data) < 100 and i%10 ==0:\n",
    "            faces_data.append(resized_image)\n",
    "        i = i + 1 \n",
    "        cv2.putText(frame,str(len(faces_data)),(50,50),cv2.FONT_HERSHEY_COMPLEX,1,(50,50,255),2)\n",
    "        \n",
    "        #cv2.rectangle(image, top-left, bottom-right, color(in BGR format), thickness)\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(50,50,255),4)\n",
    "    cv2.imshow(\"frame\",frame) #window_name, material \"to view\"\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == ord('q') or len(faces_data)== 100: # press q to quite\n",
    "        break\n",
    "print(len(faces_data))\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3b2c3472-23f8-4d31-ae76-3584777a4320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 50, 50, 3)\n",
      "(100, 7500)\n"
     ]
    }
   ],
   "source": [
    "faces_data1 = np.asarray(faces_data)\n",
    "print(faces_data1.shape)\n",
    "\n",
    "#each 100 images flatten into a rows actually\n",
    "faces_data2 = faces_data1.reshape(100,-1) \n",
    "print(faces_data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7e310474-8a4c-4c44-a624-5655b15267c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling names\n",
    "#creating first time when there is no pickle file of names id there\n",
    "if 'names.pkl' not in os.listdir('data/'):\n",
    "    names = [name] * 100\n",
    "    with open('data/names.pkl', 'wb') as f:\n",
    "        pickle.dump(names, f)\n",
    "        \n",
    "#file already exist rewrite on file from last time where i stop\n",
    "else:\n",
    "    with open('data/names.pkl', 'rb') as f:\n",
    "        names = pickle.load(f)\n",
    "    names += [name] * 100  # Append new names\n",
    "    with open('data/names.pkl', 'wb') as f:\n",
    "        pickle.dump(names, f)\n",
    "\n",
    "#if collection of faces_dataset is not there in locally then create andd write down from starting\n",
    "if 'faces_data.pkl' not in os.listdir('data/'):\n",
    "    with open('data/faces_data.pkl','wb') as f:\n",
    "        pickle.dump(faces_data2,f)\n",
    "\n",
    "#if collection of faces_datset is already exists\n",
    "else:\n",
    "    with open('data/faces_data.pkl','rb') as f:\n",
    "        faces = pickle.load(f)\n",
    "    faces = np.append(faces,faces_data2, axis = 0) #storing data after flatten all the 100 images (100, 50, 50, 3) to (100, 7500) \n",
    "    with open('data/faces_data.pkl','wb') as f:\n",
    "        pickle.dump(faces,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b9f59-0d03-4eb3-9769-ad9b8439490a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
